# k8s 
#k8s #install

## 本地通过虚拟机安装K8s

账号 qpzm7903  密码 0619

节点数量3


安装软件 openssh-server  、 network-tools

sudo apt install docker.io
docker run hello-world

sudo docker install 


配置节点



参考 https://developer.aliyun.com/mirror/kubernetes


### 节点/master
```shell
apt update
apt install docker.io
apt-get update && apt-get install -y apt-transport-https
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl

```


### 问题以及解决和参考
安装软件


初始化master节点
sudo kubeadm init --pod-network-cidr 10.244.0.0/16

使用Flannel网络组件，所以网段是默认的

失败，原因如下
```shell
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-apiserver:v1.23.4: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-controller-manager:v1.23.4: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-scheduler:v1.23.4: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-proxy:v1.23.4: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/pause:3.6: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/etcd:3.5.1-0: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/coredns/coredns:v1.8.6: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
```



也可以通过 kubeadm config images list查看所需镜像

通过手动从国内镜像下载，并打上标签
```shell
sudo docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.23.4
sudo docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.23.4  k8s.gcr.io/kube-apiserver:v1.23.4

sudo docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.23.4
sudo docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.23.4  k8s.gcr.io/kube-controller-manager:v1.23.4

sudo docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.23.4
sudo docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.23.4  k8s.gcr.io/kube-scheduler:v1.23.4

sudo docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.23.4
sudo docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.23.4  k8s.gcr.io/kube-proxy:v1.23.4

sudo docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6
sudo docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6  k8s.gcr.io/pause:3.6

sudo docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.1-0
sudo docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.1-0  k8s.gcr.io/etcd:3.5.1-0

sudo docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.6
sudo docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.6  k8s.gcr.io/coredns/coredns:v1.8.6
```



初始化并手动指定版本
sudo kubeadm init --kubernetes-version=1.23.4 --pod-network-cidr 10.244.0.0/16

#### 提示 10248 healthz失败问题
参考 https://www.myfreax.com/how-to-solve-dial-tcp-127-0-0-1-10248-connect-connection-refused-during-kubeadm-init-initialization/

修改如下
```shell
echo '{"exec-opts": ["native.cgroupdriver=systemd"]}' | sudo tee /etc/docker/daemon.json
sudo systemctl daemon-reload
sudo systemctl restart docker
sudo systemctl restart kubelet
```


未解决 参考 https://stackoverflow.com/questions/54728254/kubernetes-kubeadm-init-fails-due-to-dial-tcp-127-0-0-110248-connect-connecti
```shell
sudo mkdir /etc/docker
cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
sudo swapoff -a  
sudo sed -i '/ swap / s/^/#/' /etc/fstab

sudo systemctl enable docker
sudo systemctl daemon-reload
sudo systemctl restart docker
sudo systemctl restart kubelet

sudo kubeadm init


sudo systemctl daemon-reload
sudo systemctl restart docker
sudo systemctl restart kubelet
```


参考https://stackoverflow.com/questions/70229371/kubelet-service-is-not-running-it-seems-like-the-kubelet-isnt-running-or-healt
```shell
sudo swapoff -a  
sudo sed -i '/ swap / s/^/#/' /etc/fstab

sudo kubeadm reset
sudo systemctl daemon-reload
sudo systemctl restart docker
sudo systemctl restart kubelet
```


然后提示
```shell
our Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.127.128:6443 --token f0n6ga.xk01owg2el7du7jn --discovery-token-ca-cert-hash sha256:8dd5ae563531305cf43660b5ee28920470fde32028105d6d20f0b19b2723edaf 
```


最好切换到root用户操作


#### node节点加入集群失败，提示

[kubelet-check] Initial timeout of 40s passed.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp 127.0.0.1:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp 127.0.0.1:10248: connect: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get "http://localhost:10248/healthz": dial tcp 127.0.0.1:10248: connect: connection refused.


建议来一波
```shell
sudo mkdir /etc/docker
cat <<EOF | sudo tee /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
sudo swapoff -a  
sudo sed -i '/ swap / s/^/#/' /etc/fstab

sudo systemctl enable docker
sudo systemctl daemon-reload
sudo systemctl restart docker
sudo systemctl restart kubelet

kubeadm reset
kubeadm join ...
```



#### 验证 
在master节点上查看

```shell
root@ubuntu:/home/qpzm7903# kubectl get nodes -o wide
NAME         STATUS     ROLES                  AGE    VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
masternode   NotReady   <none>                 74m    v1.23.4   192.168.127.129   <none>        Ubuntu 20.04.3 LTS   5.4.0-104-generic   docker://20.10.7
qpzm7903     NotReady   <none>                 7m3s   v1.23.4   192.168.127.131   <none>        Ubuntu 20.04.3 LTS   5.4.0-104-generic   docker://20.10.7
ubuntu       NotReady   control-plane,master   87m    v1.23.4   192.168.127.128   <none>        Ubuntu 20.04.3 LTS   5.13.0-35-generic   docker://20.10.7

```
为什么这里的节点是NotReady


## 无法生成pod
0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 2 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn't tolerate.

为什么刚创建的集群，竟然都是taint节点？
可以使用
kubectl describe node node_name 查看详细信息

master节点上
```
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

参考
https://www.cnblogs.com/cao-lei/p/14448052.html


```
vi /etc/docker/daemon.json 


### node节点提示找不到admin.conf
```shell
root@qpzm7903:/home/qpzm7903# kubectl get nodes
W0312 04:31:41.339394   33885 loader.go:221] Config not found: /etc/kubernetes/admin.conf
The connection to the server localhost:8080 was refused - did you specify the right host or port?
root@qpzm7903:/home/qpzm7903# mv /etc/kubernetes/kubelet.conf /etc/kubernetes/admin.conf
root@qpzm7903:/home/qpzm7903# kubectl get nodes
NAME       STATUS     ROLES                  AGE   VERSION
qpzm7903   NotReady   <none>                 61s   v1.23.4
ubuntu     Ready      control-plane,master   23m   v1.23.4
root@qpzm7903:/home/qpzm7903# 

```

### node节点一直not ready

```
kubectl describe node node_name
root@ubuntu:/home/qpzm7903/k8s/demo/service# kubectl describe node qpzm7903
Name:               qpzm7903
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=qpzm7903
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 11 Mar 2022 20:31:18 -0800
Taints:             node.kubernetes.io/not-ready:NoExecute
                    node.kubernetes.io/not-ready:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  qpzm7903
  AcquireTime:     <unset>
  RenewTime:       Fri, 11 Mar 2022 20:33:50 -0800
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason     Message
  ----             ------  -----------------                 ------------------                ------     -------
  MemoryPressure   False   Fri, 11 Mar 2022 20:31:28 -0800   Fri, 11 Mar 2022 20:31:18 -0800   KubeletHasSkubelet has sufficient memory available
  DiskPressure     False   Fri, 11 Mar 2022 20:31:28 -0800   Fri, 11 Mar 2022 20:31:18 -0800   KubeletHasNkubelet has no disk pressure
  PIDPressure      False   Fri, 11 Mar 2022 20:31:28 -0800   Fri, 11 Mar 2022 20:31:18 -0800   KubeletHasSkubelet has sufficient PID available
  Ready            False   Fri, 11 Mar 2022 20:31:28 -0800   Fri, 11 Mar 2022 20:31:18 -0800   KubeletNotRcontainer runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: netwoeady: cni config uninitialized

```


在对应节点上使用 
```
journalctl -u kubelet

ar 12 04:37:08 qpzm7903 kubelet[33624]: E0312 04:37:08.100875   33624 kuberuntime_sandbox.go:70] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = failed pulling >
Mar 12 04:37:08 qpzm7903 kubelet[33624]: E0312 04:37:08.100944   33624 kuberuntime_manager.go:832] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = failed pulling >
Mar 12 04:37:08 qpzm7903 kubelet[33624]: E0312 04:37:08.101045   33624 pod_workers.go:919] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"kube-proxy-lhrjl_kube-s>
Mar 12 04:37:08 qpzm7903 kubelet[33624]: E0312 04:37:08.426441   33624 kubelet.go:2347] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotRead>
Mar 12 04:37:12 qpzm7903 kubelet[33624]: I0312 04:37:12.959665   33624 cni.go:240] "Unable to update cni config" err="no networks found in /etc/cni/net.d"


```




### 配置
not ready 的节点
```shell
root@masternode:/home/qpzm7903# kubectl config view
apiVersion: v1
clusters: null
contexts: null
current-context: ""
kind: Config
preferences: {}
users: null

```


ready的master节点
```shell

 kubectl config view

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://192.168.127.128:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED

```


### 镜像配置
 内容如下： 
{

 "registry-mirrors": ["https://xx4bwyg2.mirror.aliyuncs.com",  "https://registry.docker-cn.com", "http://hub-mirror.c.163.com", "https://docker.mirrors.ustc.edu.cn"]

	}
使配置生效
systemctl daemon-reload 
重启
Docker systemctl restart docker


## log
#k8s #log

kubectl logs pod_name
kubectl logs -f pod_name

kubectl describe pods pod_name

journalctl -u kubelet

## 使用 centos虚拟机尝试
宿主机cpu信息
```
AMD64 Family 25 Model 33 Stepping 0 AuthenticAMD ~4575 Mhz
```

### 安装必备软件

镜像源设置
```shell
# 备份

mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak


wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo


```
openssh-server